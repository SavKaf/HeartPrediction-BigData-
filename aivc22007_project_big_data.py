# -*- coding: utf-8 -*-
"""aivc22007_Project_Big_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OPfGAfVk-pIPiQ1oxVN4Nv5sIr-KhO2V

# Project for Big Data Management and Visualization - 2023
     Kaftantzis Savvas - aivc22007
     Dataset: Predict Heart Disease with Logistic Regression
     URL: https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?select=framingham.csv
"""

# The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).
# The dataset provides the patients’ information. It includes over 4.238 records and 15 attributes.
# Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.
# For CHD (10 year risk of coronary heart disease): '1' means 'Yes', '0' means 'No'.

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('dataset.csv')  # Import the csv File, using 'read_csv' Function from pandas Library

df.head()  # Check the Head (First 5 rows)

df.shape   # Number of Rows and Columns

df.info()  # Information about the DataFrame

df.describe().transpose() # Returns a summarized statistics of the series

# Check if we have got null values
df.isnull().sum()

# Let's Visualize it, just to have it in a plot.
# The most NA Values are in 'glucose' column.
sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')

""".

# We will let the missing values as it is and we won't use the average of the values in each column or anything else, but leave them as they are and run the algorithms with the exact data we have. Τhis means that we will have to use some methods so that the algorithms run correctly, as we will see below.

.

# Visualizations

Let's continue on by visualizing some more of the data to se more info about them!
"""

plt.title('Gender')
sns.countplot(data=df,x='male',hue='TenYearCHD')

"""We can see that we have a larger number of women who don't have heart disease, than men."""

plt.title('Whether or not the patient had previously had a stroke')
sns.countplot(data=df,x='prevalentStroke')

"""So we mainly got, non stroke patients."""

plt.figure(figsize=(13,6))
plt.title('CHD by Age')
sns.countplot(data=df,x='age',hue='TenYearCHD')

"""The age is between 32 and 70 year's old. And we can take a look in statistics by age"""

plt.title('CHD by Hypertensive')
sns.countplot(data=df,x='prevalentHyp',hue='TenYearCHD')

plt.title('The number of cigarettes that the person smoked on average in one day')
sns.countplot(data=df,y='cigsPerDay')

"""So we see that the patients basically either don't smoke any cigarettes, or smoke 15-20 cigarettes a day."""

plt.title('Whether or not the patient had diabetes and CHD')
sns.countplot(data=df,x='diabetes',hue='TenYearCHD')

df['totChol'].mean()
sns.displot(x='totChol',data=df,kde=True)

plt.title('10 year risk of coronary heart disease CHD (1=Yes/0=No)')
sns.countplot(data=df,x='TenYearCHD')

"""We have a bigger team that they don't have CHD.

# Correlation
"""

# This method calculates the relationship between each column in our data set.

df.corr()['TenYearCHD'][:-1].sort_values(ascending=False)

#Plot this correlation
df.corr()['TenYearCHD'][:-1].plot(kind='bar')

""".

# Building a Logistic Regression model

Let's start by splitting our data into a training set and test set. (We have Binary logistic regression)
"""

from sklearn.model_selection import train_test_split

# y is the value we want to make the prediction
X = df.drop('TenYearCHD',axis=1) # independent variables
y = df['TenYearCHD']             # dependent variable

# We split the data into random training and test set.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer

# Imputer for completing missing values with simple strategies
# We select a strategy to ignore missing values

imputer = SimpleImputer(strategy='constant', fill_value=None)

"""Fit and transform the training set"""

X_train_imputed = imputer.fit_transform(X_train)

X_test_imputed = imputer.transform(X_test)

"""Train and fit a logistic regression model on the training set"""

# Because i got warning message
#'You received indicates that the logistic regression model reached the maximum number of iterations before convergence'
# I will Increase the maximum number of iterations and
# Try a different solver
# I will choose 'liblinear' which is a linear classifier for large-scale machine learning tasks.

log = LogisticRegression(max_iter=1000,solver='liblinear')

log.fit(X_train_imputed,y_train) # The fit() method trains the logistic regression model on the provided training data,
                                 # learning the optimal coefficients that minimize the logistic loss function.

"""Now predict values for the testing data."""

predictions = log.predict(X_test_imputed) # Make predictions using X_test_imputed

"""Create a classification report for the model"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Used to show the precision, recall, F1 Score
print(classification_report(y_test,predictions))

"""  - Precision: It is a measure of correctness that is achieved in true prediction. How many predictions are actually positive out of all the total positive predicted.
  - Recall: How many observations of positive class are actually predicted as positive.
  - F-measure / F1-Score: F1 score sort of maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low.
"""

#Used for evaluating the performance of a classification mode
df_matrix = confusion_matrix(y_test,predictions)
print('Confusion matrix \n', df_matrix)

"""True Positive(TP)   =  1085   [The predicted value is Posotive and it's Positive]
    False Positive(FP)  =  12     [The predicted value is Posotive but is False - Type I Error]
    True Negative(TN)   =  13     [The predicted value is Negative and it's Negative]
    False Negative(FN)  =  162    [The predicted value is Negative but it's Positive - Type II Error]

Confusion Matrix --> Heatmap
"""

sns.heatmap(df_matrix,cmap='coolwarm',annot=True,linewidths=1,fmt='d')

# Measure the model performance of all the predictions made

logistic_regression_accuracy = accuracy_score(y_test,predictions)
print(logistic_regression_accuracy)

"""It is a measure of correctness that is achieved in true prediction.

   - Accuracy = (TP+TN) / (TP+TN+FP+FN) = Correct Predictions / Total Predictions = 86%

.

.

# K Nearest Neighbors Project - Solution

Let's now make classifications, predictions with kNN algorithm and comparison with the above model, Logistic Regression.

- Let's first Standardize the Variables

Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.
"""

from sklearn.preprocessing import StandardScaler

# StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance.
# Unit variance means dividing all the values by the standard deviation.

#Create an instance of StandardScaler

scaler = StandardScaler()

# Fit the scaler to your data (calculate mean and standard deviation)
scaler.fit(df.drop('TenYearCHD',axis=1))

scaled_features = scaler.transform(df.drop('TenYearCHD',axis=1)) # I transform only the columns i want (X), except the column
                                                                 # that i want to predict

df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
df_feat.head()

""".

Now we will split our data in train and test set.
"""

from sklearn.model_selection import train_test_split

#Split the model in train and test set
X_train2, X_test2, y_train2, y_test2 = train_test_split(scaled_features,
                                                    df['TenYearCHD'],
                                                    test_size=0.3,
                                                    random_state=42)

"""# kNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer

# Perform kNN imputation on the training and testing sets.
# We do that because we have got Null values and we don't want to fill them and run the algorithms in the dataset
# as it is

imputer = KNNImputer(n_neighbors=1)
X_train_imputed2 = imputer.fit_transform(X_train2)
X_test_imputed2 = imputer.transform(X_test2)

# Initialize and train the kNN classifier.
# We will start with k = 1

knn = KNeighborsClassifier(n_neighbors=1)

#Fit our Model

knn.fit(X_train_imputed2,y_train2)

pred = knn.predict(X_test_imputed2)

"""Predictions and Evaluations"""

from sklearn.metrics import classification_report,confusion_matrix, accuracy_score

print(confusion_matrix(y_test2,pred))  # Print confusion matrix to define the performance of a classification algorithm

print(classification_report(y_test2,pred))

# Measure the model performance of all the predictions made

print(accuracy_score(y_test2,pred))

"""# Let's see if we can pick a good value for k, using elbow method"""

# use the elbow method to pick a good K Value
import numpy as np

error_rate = []


for i in range(1,40):

    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train_imputed2,y_train2)
    pred_i = knn.predict(X_test_imputed2)
    error_rate.append(np.mean(pred_i != y_test2))

# Plot error rate vs k
plt.figure(figsize=(9,5))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

"""Here we can see that that after k>14 the error rate just tends to hover around 0.15-0.08.
Let's retrain the model with lowest k values, and check the classification report!
"""

# FIRST A QUICK COMPARISON TO OUR ORIGINAL K=1
knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train_imputed2,y_train2)
pred = knn.predict(X_test_imputed2)

print('WITH K=1')
print('\n')
print(confusion_matrix(y_test2,pred))
print('\n')
print(classification_report(y_test2,pred))
print('\n')
print(accuracy_score(y_test2,pred))

#Now for k = 19 (for k =17 we got the same results)
knn = KNeighborsClassifier(n_neighbors=19)

knn.fit(X_train_imputed2,y_train2)
pred = knn.predict(X_test_imputed2)

print('WITH K=19')
print('\n')
print(confusion_matrix(y_test2,pred))
print('\n')
print(classification_report(y_test2,pred))
print('\n')
knn_accuracy = accuracy_score(y_test2,pred)
print(knn_accuracy)

"""For k = 19 and for k = 17, the Final kNN Accuracy it's the same.

.

# So we can see the Final predictions from Logistic Regression and kNN algorithm
"""

print('Accuracy Score for kNN algorithm (k=19) = ', knn_accuracy)
print('\n')
print('Accuracy Score for Logistic Regression =', logistic_regression_accuracy)

"""."""

# Plot the comparison of Accuracy scores
accuracy_scores = [logistic_regression_accuracy, knn_accuracy]
models = ['Logistic Regression', 'kNN']

plt.plot(models, accuracy_scores, marker='o')
plt.xlabel('Models')
plt.ylabel('Accuracy Score')
plt.title('Comparison of Accuracy Scores')
plt.ylim(0, 1.0)  # Set the y-axis limits between 0 and 1
plt.show()

""".

.

So we can see from the two final results that logistic regression has a better prediction rate than kNN, in the specific dataset

.

.

.

# Measuring algorithm execution times by growing the dataset

We will import again the same dataset, in different Dataframe, and Duplicate the Dataset by 30 times
"""

dataset = pd.read_csv("dataset.csv")

bigdata = pd.concat([dataset] * 30, ignore_index=True)

#The bigdata variable, now contains the original dataset repeated 30 times.
bigdata.shape

# To measure the execution time for the two algorithms (logistic regression and kNN),
# we will use the 'time' module in Python.
import time

# And now in the same way that we ran the algorithms above,
# this is how we will do and now, we will simply count the time as well.

# Logistic Regression Model #
# y is the value we want to make the prediction
start_time = time.time()
X2 = bigdata.drop('TenYearCHD',axis=1)
y2 = bigdata['TenYearCHD']
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=101)
X2_train_imputed = imputer.fit_transform(X2_train)
X2_test_imputed = imputer.transform(X2_test)
log2 = LogisticRegression(max_iter=1000,solver='liblinear')

# Execution time
#start_time = time.time()
log.fit(X2_train_imputed,y2_train)
end_time = time.time()

log_reg_time = end_time - start_time

# kNN Algorithm #
start_time2 = time.time()
scaler2 = StandardScaler()
scaler2.fit(bigdata.drop('TenYearCHD',axis=1))

scaled_features2 = scaler2.transform(bigdata.drop('TenYearCHD',axis=1))
bigdata_feat = pd.DataFrame(scaled_features2,columns=bigdata.columns[:-1])

X2_train2, X2_test2, y2_train2, y2_test2 = train_test_split(scaled_features2,
                                                    bigdata['TenYearCHD'],
                                                    test_size=0.3,
                                                    random_state=42)
imputer2 = KNNImputer(n_neighbors=1)
X2_train_imputed2 = imputer2.fit_transform(X2_train2)
X2_test_imputed2 = imputer2.transform(X2_test2)

# kNN
knn2 = KNeighborsClassifier(n_neighbors=1)

# Execute Time
#start_time = time.time()
knn2.fit(X2_train_imputed2,y2_train2)
end_time2 = time.time()

knn_time = end_time2 - start_time2

# Print the execution times
print("Logistic Regression Time:", log_reg_time)
print("kNN Time:", knn_time)

"""As we can see both algorithms take the same time to complete, about 3 minutes.

With a few seconds faster kNN from Logistic Regression algorithm.

But the difference is minimal where it doesn't give us any engine of choice due to speed.

.

.

# Any help or source for codes and methodologies, i needed, i used:              
- Articles from Medium site
- Stack Overflow
- Udemy Course ''Python for Data Science and Machine Learning Bootcamp''.
"""